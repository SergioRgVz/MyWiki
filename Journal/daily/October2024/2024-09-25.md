- [ ] Montar obsidian bien en portátil de Herta
- [ ] Hacer script para de cada máscara de persona del dataset de COCO, pasarle nuestro modelo de segmentación de ropa y que anote en el formato COCO en otro fichero
	- [x] Hacer script para coger los datos automáticos de una imagen y pintar ese segmentado
	- [x] Adaptar script para hacer un crop de todas las personas segmentadas
	- [x] Adaptar script para realizar una segmentación de ropa sobre esas personas segmentadas
	- [ ] Adaptar script para escribir etiquetado de esta nueva segmentación en formato yolo
	- [ ] Adaptarlo para que lo escriba en formato COCO en un JSON aparte

```python
import cv2
import numpy as np
from pycocotools.coco import COCO
import os
from ultralytics import YOLO
from PIL import Image
import json
import datetime

def calculate_iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[0] + box1[2], box2[0] + box2[2])
    y2 = min(box1[1] + box1[3], box2[1] + box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = box1[2] * box1[3]
    area2 = box2[2] * box2[3]
    
    iou = intersection / float(area1 + area2 - intersection)
    return iou

def resize_with_padding(image, target_size):
    w, h = image.size
    ratio = min(target_size[0] / w, target_size[1] / h)
    new_size = (int(w * ratio), int(h * ratio))
    resized_image = image.resize(new_size, Image.LANCZOS)
    new_image = Image.new('RGB', target_size, (0, 0, 0))
    new_image.paste(resized_image, ((target_size[0] - new_size[0]) // 2, (target_size[1] - new_size[1]) // 2))
    pad_x = (target_size[0] - new_size[0]) // 2
    pad_y = (target_size[1] - new_size[1]) // 2
    return new_image, pad_x, pad_y, ratio

def load_image_and_coco_annotations(image_path, annotations_path):
    coco = COCO(annotations_path)
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    file_name = os.path.basename(image_path)
    img_id = next((id for id in coco.getImgIds() if coco.loadImgs(id)[0]['file_name'] == file_name), None)
    
    if img_id is None:
        raise ValueError(f"No se encontró la imagen {file_name} en las anotaciones COCO.")
    
    return image, coco, img_id

def get_annotations(coco, img_id, categories):
    cat_ids = coco.getCatIds(catNms=categories)
    ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids)
    return coco.loadAnns(ann_ids)

def associate_bags_with_people(anns_persona, anns_bolsa, iou_threshold=0.1):
    personas_con_bolsas = []
    bolsas_asociadas = set()
    
    for ann_persona in anns_persona:
        bbox_persona = ann_persona['bbox']
        bolsas_de_persona = []
        
        for ann_bolsa in anns_bolsa:
            if ann_bolsa['id'] in bolsas_asociadas:
                continue
            
            bbox_bolsa = ann_bolsa['bbox']
            iou = calculate_iou(bbox_persona, bbox_bolsa)
            
            if iou > iou_threshold:
                bolsas_de_persona.append(ann_bolsa)
                bolsas_asociadas.add(ann_bolsa['id'])
        
        personas_con_bolsas.append((ann_persona, bolsas_de_persona))
    
    return personas_con_bolsas

def apply_yolo_with_confidence(model, image, mask, conf_threshold=0.25):
    if not isinstance(mask, np.ndarray):
        raise ValueError("La máscara debe ser un array numpy, no un float.")

    masked_image = image.copy()
    masked_image[mask == 0] = [0, 0, 0]

    orig_image = Image.fromarray(masked_image)
    resized_image, pad_x, pad_y, scale = resize_with_padding(orig_image, (640, 640))
    
    results = model(resized_image, conf=conf_threshold)
    
    segmentation_mask = np.zeros(image.shape[:2], dtype=np.uint8)
    colored_mask = np.zeros(image.shape, dtype=np.uint8)
    overlay_image = image.copy()
    
    class_colors = {}
    
    for result in results:
        masks = result.masks
        boxes = result.boxes.xyxy
        classes = result.boxes.cls
        confs = result.boxes.conf
        names = result.names
        
        if masks is not None:
            for mask_yolo, box, cls, conf in zip(masks.data, boxes, classes, confs):
                class_name = names[int(cls)]
                
                if class_name not in class_colors:
                    class_colors[class_name] = tuple(np.random.randint(0, 255, 3).tolist())
                color = class_colors[class_name]
                
                mask_image = Image.fromarray((mask_yolo.cpu().numpy() * 255).astype('uint8')).convert('L')
                mask_image = mask_image.resize((640, 640))
                
                orig_w, orig_h = image.shape[1], image.shape[0]
                mask_orig = mask_image.crop((pad_x, pad_y, 640 - pad_x, 640 - pad_y))
                mask_orig = mask_orig.resize((orig_w, orig_h))
                mask_np = np.array(mask_orig)
                
                mask_np = cv2.resize(mask_np, (mask.shape[1], mask.shape[0]))
                mask_np = np.logical_and(mask_np > 0, mask > 0).astype(np.uint8)
                
                segmentation_mask[mask_np > 0] = int(cls) + 1
                colored_mask[mask_np > 0] = color
                
                overlay_image[mask_np > 0] = overlay_image[mask_np > 0] * 0.5 + np.array(color) * 0.5
                
                # Ajustar las coordenadas del bounding box
                box = box.tolist()
                box[0] = (box[0] - pad_x) / scale
                box[1] = (box[1] - pad_y) / scale
                box[2] = (box[2] - pad_x) / scale
                box[3] = (box[3] - pad_y) / scale
                
                cv2.rectangle(overlay_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)
                cv2.putText(overlay_image, f"{class_name} {conf:.2f}", (int(box[0]), int(box[1] - 10)),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    return overlay_image, segmentation_mask, colored_mask, results

def extract_and_process_segments(image, coco, personas_con_bolsas, model, conf_threshold, output_dir):
    full_segmentation = image.copy()
    all_results = []
    
    for i, (ann_persona, bolsas) in enumerate(personas_con_bolsas):
        mask_persona = coco.annToMask(ann_persona)
        
        for bolsa in bolsas:
            mask_bolsa = coco.annToMask(bolsa)
            mask_persona = np.logical_or(mask_persona, mask_bolsa)
        
        y, x = np.where(mask_persona)
        top, bottom, left, right = y.min(), y.max(), x.min(), x.max()
        
        segmento = image[top:bottom+1, left:right+1].copy()
        mask_segment = mask_persona[top:bottom+1, left:right+1]
        
        if segmento.size == 0 or mask_segment.size == 0:
            print(f"Advertencia: Segmento vacío para la persona {i}. Saltando...")
            continue
        
        overlay_image, segmentation_mask, colored_mask, results = apply_yolo_with_confidence(model, segmento, mask_segment, conf_threshold)
        
        all_results.extend(results)
        
        cv2.imwrite(os.path.join(output_dir, f'overlay_persona_{"con" if bolsas else "sin"}_bolsa_{i}.png'), cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR))
        cv2.imwrite(os.path.join(output_dir, f'mask_persona_{"con" if bolsas else "sin"}_bolsa_{i}.png'), segmentation_mask * 255)
        cv2.imwrite(os.path.join(output_dir, f'colored_mask_persona_{"con" if bolsas else "sin"}_bolsa_{i}.png'), cv2.cvtColor(colored_mask, cv2.COLOR_RGB2BGR))
        
        full_segmentation[top:bottom+1, left:right+1] = overlay_image
        cv2.rectangle(full_segmentation, (left, top), (right, bottom), (255, 0, 0), 2)
        
        unique_classes = np.unique(segmentation_mask)
        print(f"Clases detectadas en el segmento {i}: {[model.names[cls-1] for cls in unique_classes if cls > 0]}")
    
    return full_segmentation, all_results

def create_coco_annotations(image_path, results, output_file):
    coco_output = {
        "info": {
            "description": "Dataset generado con YOLOv8",
            "url": "",
            "version": "1.0",
            "year": datetime.datetime.now().year,
            "contributor": "",
            "date_created": datetime.datetime.now().isoformat()
        },
        "licenses": [{"url": "", "id": 1, "name": "Unknown"}],
        "images": [],
        "annotations": [],
        "categories": []
    }
    
    image = Image.open(image_path)
    image_id = 1
    orig_width, orig_height = image.size
    coco_output["images"].append({
        "id": image_id,
        "width": orig_width,
        "height": orig_height,
        "file_name": os.path.basename(image_path),
        "license": 1,
        "flickr_url": "",
        "coco_url": "",
        "date_captured": datetime.datetime.now().isoformat()
    })
    
    processed_width, processed_height = results[0].orig_shape
    
    scale_x = orig_width / processed_width
    scale_y = orig_height / processed_height
    
    category_map = {}
    category_id = 1
    
    annotation_id = 1
    for result in results:
        for i in range(len(result.boxes)):
            class_id = int(result.boxes.cls[i])
            class_name = result.names[class_id]
            confidence = float(result.boxes.conf[i])
            bbox = result.boxes.xyxy[i].tolist()
            
            bbox_orig = [
                bbox[0] * scale_x,
                bbox[1] * scale_y,
                (bbox[2] - bbox[0]) * scale_x,
                (bbox[3] - bbox[1]) * scale_y
            ]
            
            if result.masks is not None:
                mask = result.masks.data[i].cpu().numpy()
                mask = cv2.resize(mask, (orig_width, orig_height))
                contours, _ = cv2.findContours(
                    (mask * 255).astype(np.uint8),
                    cv2.RETR_EXTERNAL,
                    cv2.CHAIN_APPROX_SIMPLE
                )
                segmentation = [contour.flatten().tolist() for contour in contours]
            else:
                segmentation = []
            
            if class_name not in category_map:
                category_map[class_name] = category_id
                coco_output["categories"].append({
                    "id": category_id,
                    "name": class_name,
                    "supercategory": "object"
                })
                category_id += 1
            
            annotation = {
                "id": annotation_id,
                "image_id": image_id,
                "category_id": category_map[class_name],
                "bbox": bbox_orig,
                "area": bbox_orig[2] * bbox_orig[3],
                "segmentation": segmentation,
                "iscrowd": 0,
                "score": confidence
            }
            
            coco_output["annotations"].append(annotation)
            annotation_id += 1
    
    with open(output_file, 'w') as f:
        json.dump(coco_output, f, separators=(',', ':'))

def main():
    CONFIDENCE_THRESHOLD = 0.15
    IMAGE_PATH = '/workspace/COCO_DATASET/scripts/000000001000.jpg'
    ANNOTATIONS_PATH = '/workspaces/herta/COCO_DATASET/annotations_trainval2017/annotations/instances_val2017.json'
    MODEL_PATH = "/workspace/COCO_DATASET/scripts/segment_clothes.pt"
    OUTPUT_DIR = 'segmentos_extraidos'
    COCO_OUTPUT_FILE = 'output_annotations.json'
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    model = YOLO(MODEL_PATH)
    
    image, coco, img_id = load_image_and_coco_annotations(IMAGE_PATH, ANNOTATIONS_PATH)
    
    anns_persona = get_annotations(coco, img_id, ['person'])
    anns_bolsa = get_annotations(coco, img_id, ['backpack', 'handbag', 'suitcase'])
    
    personas_con_bolsas = associate_bags_with_people(anns_persona, anns_bolsa)
    
    full_segmentation, all_results = extract_and_process_segments(
        image, coco, personas_con_bolsas, model, CONFIDENCE_THRESHOLD, OUTPUT_DIR
    )
    
    full_segmentation_path = os.path.join(OUTPUT_DIR, 'full_segmentation.png')
    cv2.imwrite(full_segmentation_path, cv2.cvtColor(full_segmentation, cv2.COLOR_RGB2BGR))
    print(f"Segmentación completa guardada en: {full_segmentation_path}")
    # Create and save COCO annotations
    create_coco_annotations(IMAGE_PATH, all_results, os.path.join(OUTPUT_DIR, COCO_OUTPUT_FILE))
    print(f"Anotaciones COCO guardadas en: {os.path.join(OUTPUT_DIR, COCO_OUTPUT_FILE)}")
    
    # Print summary
    print("\nResumen del procesamiento:")
    print(f"Imagen original: {os.path.basename(IMAGE_PATH)}")
    print(f"Número de personas detectadas: {len(personas_con_bolsas)}")
    print(f"Número total de detecciones: {sum(len(result.boxes) for result in all_results)}")

if __name__ == "__main__":
    main()
```
Este es para anotar pero tengo problemas con las anotaciones.json de segmentación ya que hace cosas bastante raras

```python
import json
import cv2
import numpy as np
import random
import os

def load_coco_annotations(json_file):
    with open(json_file, 'r') as f:
        data = json.load(f)
    print(f"Loaded {len(data['annotations'])} annotations and {len(data['categories'])} categories.")
    return data

def draw_bbox(image, bbox, color, thickness=2):
    x, y, w, h = map(int, bbox)
    cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)

def draw_segmentation(image, segmentation, color, alpha=0.5):
    mask = np.zeros(image.shape[:2], dtype=np.uint8)
    for seg in segmentation:
        pts = np.array(seg).reshape((-1, 1, 2)).astype(np.int32)
        cv2.fillPoly(mask, [pts], 1)
    
    color_mask = np.zeros_like(image)
    color_mask[mask == 1] = color
    
    cv2.addWeighted(image, 1, color_mask, alpha, 0, image)

def visualize_and_save_coco_annotations(image_path, annotations_file, output_path):
    # Cargar la imagen
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"No se pudo cargar la imagen: {image_path}")
    print(f"Imagen cargada con dimensiones: {image.shape}")
    
    # Crear una copia de la imagen original para dibujar las anotaciones
    annotated_image = image.copy()
    
    # Cargar las anotaciones
    coco_data = load_coco_annotations(annotations_file)
    
    # Verificar que las dimensiones de la imagen coincidan con las anotaciones
    img_info = coco_data['images'][0]
    img_height, img_width = image.shape[:2]
    ann_height, ann_width = img_info['height'], img_info['width']
    
    # Calcular factores de escala
    scale_x = img_width / ann_width
    scale_y = img_height / ann_height
    
    # Crear un diccionario de colores para las categorías
    category_colors = {cat['id']: tuple(random.randint(0, 255) for _ in range(3)) 
                       for cat in coco_data['categories']}
    
    # Dibujar las anotaciones
    annotations_drawn = 0
    for ann in coco_data['annotations']:
        color = category_colors[ann['category_id']]
        
        # Obtener y escalar el bounding box
        bbox = ann['bbox']
        if len(bbox) == 4:  # [x, y, width, height]
            x, y, w, h = bbox
            x1, y1 = int(x * scale_x), int(y * scale_y)
            x2, y2 = int((x + w) * scale_x), int((y + h) * scale_y)
        elif len(bbox) == 5:  # [x1, y1, x2, y2, score]
            x1, y1, x2, y2, _ = bbox
            x1, y1 = int(x1 * scale_x), int(y1 * scale_y)
            x2, y2 = int(x2 * scale_x), int(y2 * scale_y)
        else:
            print(f"Formato de bounding box no reconocido: {bbox}")
            continue
        
        # Dibujar el bounding box
        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)
        
        # Dibujar la segmentación (si existe)
        if ann['segmentation']:
            segmentation_scaled = [[int(coord * scale_x if i % 2 == 0 else coord * scale_y) 
                                    for i, coord in enumerate(seg)] 
                                   for seg in ann['segmentation']]
            draw_segmentation(annotated_image, segmentation_scaled, color)
        
        # Añadir etiqueta
        category = next(cat for cat in coco_data['categories'] if cat['id'] == ann['category_id'])
        label = f"{category['name']} {ann['score']:.2f}"
        cv2.putText(annotated_image, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        annotations_drawn += 1
    
    print(f"Se dibujaron {annotations_drawn} anotaciones en la imagen.")
    
    # Guardar la imagen anotada
    cv2.imwrite(output_path, annotated_image)
    print(f"Imagen anotada guardada en: {output_path}")

# Uso del script
if __name__ == "__main__":
    IMAGE_PATH = '/workspace/COCO_DATASET/scripts/000000001000.jpg'  # Ajusta esta ruta a tu imagen
    ANNOTATIONS_FILE = 'segmentos_extraidos/output_annotations.json'  # Ajusta esta ruta a tu archivo JSON
    OUTPUT_DIR = 'segmentos_extraidos'
    OUTPUT_FILE = 'imagen_anotada.jpg'
    
    # Asegurarse de que el directorio de salida existe
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Ruta completa para el archivo de salida
    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    
    visualize_and_save_coco_annotations(IMAGE_PATH, ANNOTATIONS_FILE, output_path)
```
Este es el de pintado