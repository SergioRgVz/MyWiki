```Python
import cv2
import numpy as np
from pycocotools.coco import COCO
import os
from ultralytics import YOLO
from PIL import Image
import json
import datetime
import torchvision.transforms.functional as F
import torch
from pycocotools import mask as mask_util


def calculate_iou(box1, box2):
    """
    Calculate the Intersection over Union (IoU) of two bounding boxes.

    @param box1 List of 4 numbers representing the first bounding box [x, y, width, height].
    @param box2 List of 4 numbers representing the second bounding box [x, y, width, height].
    @return float The IoU value between 0 and 1.

    If there's no intersection, the function returns 0.0.
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[0] + box1[2], box2[0] + box2[2])
    y2 = min(box1[1] + box1[3], box2[1] + box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = box1[2] * box1[3]
    area2 = box2[2] * box2[3]
    
    iou = intersection / float(area1 + area2 - intersection)
    return iou


def resize_with_padding(person, target_size):
    """
    Resize an image with padding to reach a target size while maintaining aspect ratio.

    @param person PIL.Image The input image to be resized.
    @param target_size tuple A tuple of (width, height) representing the desired output size.
    @return tuple A tuple containing:
        - PIL.Image: The resized and padded image.
        - float: The ratio used for resizing.

    Note: The function assumes a target size of 512x512 for ratio calculation.
    The cv2 and numpy libraries are used for saving the image.

    @todo Consider making the target size (512x512) a parameter instead of a hard-coded value.
    """
    w, h = person.size
    ratio = min(512 / w, 512 / h)
    new_size = (int(w * ratio), int(h * ratio))
    resized_image = person.resize(new_size, Image.LANCZOS)
    new_image = Image.new('RGB', target_size, (0, 0, 0))
    new_image.paste(resized_image, (0, 0))    
    return new_image, ratio

def load_image_and_coco_annotations(image_path, annotations_path):
    """
    Load an image and its corresponding COCO annotations.

    @param image_path str The file path to the image.
    @param annotations_path str The file path to the COCO annotations JSON file.
    @return tuple A tuple containing:
        - numpy.ndarray: The loaded image in RGB format.
        - pycocotools.coco.COCO: The COCO object with loaded annotations.
        - int: The image ID in the COCO dataset.


    @throws ValueError If the image is not found in the COCO annotations.

    @note This function requires the pycocotools, cv2, and os libraries.
    @note The image is converted from BGR to RGB color space.
    """
    coco = COCO(annotations_path)
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    file_name = os.path.basename(image_path)
    img_id = next((id for id in coco.getImgIds() if coco.loadImgs(id)[0]['file_name'] == file_name), None)
    
    if img_id is None:
        raise ValueError(f"Image not fount {file_name} in COCO's annotations.")
    
    return image, coco, img_id

def get_annotations(coco, img_id, categories):
    """
    Retrieve annotations for a specific image and categories from a COCO dataset.

    @param coco pycocotools.coco.COCO The COCO object containing the dataset annotations.
    @param img_id int The ID of the image for which to retrieve annotations.
    @param categories list A list of category names to filter the annotations.
    @return list A list of annotation dictionaries matching the specified image and categories.

    @note This function assumes that the COCO object is properly initialized with the dataset.
    @note The returned annotations are in the COCO format, which typically includes fields like
          'segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', and 'id'.
    """
    cat_ids = coco.getCatIds(catNms=categories)
    ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids)
    return coco.loadAnns(ann_ids)

def associate_bags_with_people(anns_person, anns_bags, iou_threshold=0.1):
    """
    Associate bags with people based on the Intersection over Union (IoU) of their bounding boxes.

    @param anns_person list A list of person annotations, each containing a 'bbox' key.
    @param anns_bags list A list of bag annotations, each containing a 'bbox' key and an 'id' key.
    @param iou_threshold float The minimum IoU value to consider a bag associated with a person. Default is 0.1.
    @return list A list of tuples, each containing a person annotation and a list of associated bag annotations.

    @note The function assumes the existence of a 'calculate_iou' function to compute the IoU between bounding boxes.
    @note Each bag can only be associated with one person.
    @note The bounding boxes are expected to be in the format [x, y, width, height].

    @see calculate_iou() for the IoU calculation implementation.
    """
    person_with_bags = []
    associated_bags = set()
    
    for ann_person in anns_person:
        bbox_person = ann_person['bbox']
        bags_from_person = []
        
        for ann_bag in anns_bags:
            if ann_bag['id'] in associated_bags:
                continue
            
            bbox_bag = ann_bag['bbox']
            iou = calculate_iou(bbox_person, bbox_bag)
            
            if iou > iou_threshold:
                bags_from_person.append(ann_bag)
                associated_bags.add(ann_bag['id'])
        
        person_with_bags.append((ann_person, bags_from_person))
    
    return person_with_bags

def apply_yolo_with_confidence(model, image, mask, bbox, conf_threshold=0.25):
    """
    Apply YOLO object detection to a specific region of an image, considering a mask and confidence threshold.

    @param model The YOLO model to use for detection.
    @param image numpy.ndarray The full input image.
    @param mask numpy.ndarray A binary mask of the same size as the image.
    @param bbox tuple Bounding box coordinates (left, top, right, bottom) of the region of interest.
    @param conf_threshold float Confidence threshold for YOLO detections. Default is 0.25.

    @return tuple A tuple containing:
        - numpy.ndarray: Overlay image with detection visualizations.
        - numpy.ndarray: Segmentation mask with class IDs.
        - numpy.ndarray: Colored mask representing different detected classes.
        - dict: Detailed results including masks, boxes, classes, confidences, and metadata.

    @raises ValueError If the mask is not a numpy array.

    This function performs the following steps:
    1. Extracts the region of interest from the image and mask.
    2. Applies the mask to the region of interest.
    3. Resizes the masked region for YOLO input.
    4. Runs YOLO detection on the resized image.
    5. Processes YOLO results, including resizing masks and boxes back to original size.
    6. Creates visualizations including segmentation masks and bounding boxes.
    7. Compiles detailed results into a dictionary.

    @note This function assumes the existence of a 'resize_with_padding' function.
    @note The function uses OpenCV (cv2) for drawing and image processing operations.
    @note YOLO results are expected to include masks for segmentation.

    @todo Consider adding error handling for potential issues in YOLO detection or result processing.
    @todo Optimize the visualization process for better performance with large images or numerous detections.
    """
    if not isinstance(mask, np.ndarray):
        raise ValueError("La máscara debe ser un array numpy, no un float.")

    left, top, right, bottom = bbox
    person = image[top:bottom, left:right].copy()
    person_mask = mask[top:bottom, left:right]
    
    person[person_mask == 0] = [0, 0, 0]

    person_image = Image.fromarray(person)
    resized_person, ratio = resize_with_padding(person_image, (640, 640))
    
    results = model(resized_person, conf=conf_threshold)
    
    assert len(results) == 1, "Only one output should be returned."
    result = results[0]

    height, width = person.shape[:2]
    segmentation_mask = np.zeros((height, width), dtype=np.uint8)
    colored_mask = np.zeros((height, width, 3), dtype=np.uint8)
    overlay_segment = image[top:bottom, left:right].copy()
    
    class_colors = {}
    
    result_dict = {}

    if result.masks is not None:
        new_h, new_w = int(640/ratio), int(640/ratio)
        result_dict['masks'] = F.resize(result.masks.data, (new_h, new_w), interpolation=F.InterpolationMode.NEAREST)
        result_dict['boxes'] = result.boxes.xyxy / ratio
        result_dict['classes'] = result.boxes.cls
        result_dict['confs'] = result.boxes.conf
        result_dict['names'] = result.names
        result_dict['size'] = {'height': height, 'width': width}
        result_dict['relative_position'] = { 'left': left, 'top': top, 'right': right, 'bottom': bottom }
        result_dict['categories'] = [category for _, category in results[0].names.items()]
        for mask_yolo, box, cls, conf in zip(result_dict['masks'], result_dict['boxes'], result_dict['classes'], result_dict['confs'] ):
            class_name = result_dict['names'][int(cls)]
            
            if class_name not in class_colors:
                class_colors[class_name] = tuple(np.random.randint(0, 255, 3).tolist())
            color = class_colors[class_name]
            
            # Convert mask to original size of the person
            mask_np = mask_yolo.cpu().numpy()
            mask_np = mask_np[:height, :width]
            
            mask_np = np.logical_and(mask_np > 0.5, person_mask > 0).astype(np.uint8)
            # Apply mask to segmentation mask and colored mask
            segmentation_mask[mask_np > 0] = int(cls) + 1
            colored_mask[mask_np > 0] = color
            
            overlay_segment[mask_np > 0] = overlay_segment[mask_np > 0] * 0.5 + np.array(color) * 0.5
            
            box = box.tolist()
            
            cv2.rectangle(overlay_segment, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)
            cv2.putText(overlay_segment, f"{class_name} {conf:.2f}", (int(box[0]), int(box[1] - 10)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    return overlay_segment, segmentation_mask, colored_mask, result_dict



def extract_and_process_segments(image, coco, people_with_bags, model, conf_threshold, output_dir):
    """
    @brief Extracts and processes segments from an image based on person and bag annotations.
    
    This function takes an image and annotations for people and bags, processes each segment
    using a YOLO model, and saves the results as images and detection information.
    
    @param image The input image to process.
    @param coco A COCO annotation object for creating masks.
    @param people_with_bags A list of tuples containing person annotations and associated bag annotations.
    @param model The YOLO model used for object detection.
    @param conf_threshold Confidence threshold for YOLO detections.
    @param output_dir Directory to save output images.
    
    @return A tuple containing:
            - full_segmentation: The input image with all processed segments overlaid.
            - all_results: A list of dictionaries containing detection results for each segment.
    
    @details The function performs the following steps for each person-bag pair:
             1. Creates a mask combining the person and their associated bags.
             2. Extracts the bounding box for the combined mask.
             3. Applies YOLO detection on the masked region.
             4. Saves overlay, segmentation mask, and colored mask images.
             5. Updates the full segmentation image with the processed segment.
             6. Prints detected classes for each segment.
    
    @note Empty segments are skipped with a warning message.
    """
    full_segmentation = image.copy()
    all_results = []
    
    for i, (ann_person, bags) in enumerate(people_with_bags):
        person_mask = coco.annToMask(ann_person)
        
        for bag in bags:
            bag_mask = coco.annToMask(bag)
            person_mask = np.logical_or(person_mask, bag_mask)
        
        y, x = np.where(person_mask)
        top, bottom, left, right = y.min(), y.max(), x.min(), x.max()
        
        if top == bottom or left == right:
            print(f"Warning: Empty segment for person {i}. Skipping...")
            continue
        
        overlay_image, segmentation_mask, colored_mask, result_dict = apply_yolo_with_confidence(model, image, person_mask, (left, top, right, bottom), conf_threshold)
        
        all_results.append(result_dict)
        
        cv2.imwrite(os.path.join(output_dir, f'overlay_person_{"with" if bags else "without"}_bag_{i}.png'), cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR))
        cv2.imwrite(os.path.join(output_dir, f'person_mask_{"with" if bags else "without"}_bag_{i}.png'), segmentation_mask * 255)
        cv2.imwrite(os.path.join(output_dir, f'colored_person_mask_{"with" if bags else "without"}_bag_{i}.png'), cv2.cvtColor(colored_mask, cv2.COLOR_RGB2BGR))
        
        full_segmentation[top:bottom, left:right] = overlay_image
        cv2.rectangle(full_segmentation, (left, top), (right, bottom), (255, 0, 0), 2)
        
        unique_classes = np.unique(segmentation_mask)
        print(f"Classes detected in segment {i}: {[model.names[cls-1] for cls in unique_classes if cls > 0]}")
    
    return full_segmentation, all_results

def create_coco_annotations(image_path, results, output_file):
    """
    Creates COCO-format annotations from YOLO detection results and existing annotations.

    Args:
        image_path (str): Path to the input image file.
        results (list): List of dictionaries containing YOLO detection results for each segment.
        output_file (str): Path to save the output COCO JSON file.

    This function performs the following steps:
    1. Initializes a COCO-format dictionary with metadata.
    2. Adds categories based on the detection results.
    3. Adds image information to the COCO structure.
    4. Processes each set of detection results:
       - Converts YOLO masks to binary masks.
       - Finds contours in the binary masks.
       - Transforms contours to absolute image coordinates.
       - Calculates areas and bounding boxes.
       - Creates COCO-format annotations for each detected object.
    5. Saves the COCO-format annotations to a JSON file.

    Note:
        - The function handles potential errors in mask processing and contour finding.
    Warning:
        Ensure that the input image and detection results correspond to each other.
    """
    coco_output = {
        "info": {
            "description": "Dataset generated with YOLOv8",
            "url": "",
            "version": "1.0",
            "year": datetime.datetime.now().year,
            "contributor": "",
            "date_created": datetime.datetime.now().isoformat()
        },
        "licenses": [{"url": "", "id": 1, "name": "Unknown"}],
        "images": [],
        "annotations": [],
        "categories": []
    }

    for i, category in enumerate(results[0]['categories'], 1):
        coco_output["categories"].append({
            "id": i,
            "name": category,
            "supercategory": ""
        })
    
    image = Image.open(image_path)
    image_id = 1
    orig_width, orig_height = image.size
    coco_output["images"].append({
        "id": image_id,
        "width": orig_width,
        "height": orig_height,
        "file_name": os.path.basename(image_path),
        "license": 1,
        "flickr_url": "",
        "coco_url": "",
        "date_captured": datetime.datetime.now().isoformat()
    })
    
    for i in range(len(results)):
        if len(results[i]) == 0:
            continue
        masks_yolo = results[i]['masks']
        boxes = results[i]['boxes'].tolist()
        cls = results[i]['classes'].tolist()
        conf = results[i]['confs'].tolist()
        height, width = results[i]['size']['height'], results[i]['size']['width']
        left, top, right, bottom = results[i]['relative_position']['left'], results[i]['relative_position']['top'], results[i]['relative_position']['right'], results[i]['relative_position']['bottom']

        for mask, box, cls, conf in zip(masks_yolo, boxes, cls, conf):
            width_box, height_box = box[2] - box[0], box[3] - box[1]
            absolute_box = [left + box[0], top + box[1], width_box, height_box]

            if isinstance(mask, torch.Tensor):
                mask_np = mask.cpu().numpy()
                mask_np = mask_np[:height, :width]
            elif isinstance(mask, np.ndarray):
                mask_np = mask

            if mask_np.ndim > 2:
                mask_np = mask_np.squeeze()
        
            binary_mask = (mask_np > 0.5).astype(np.uint8) * 255

            if binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0:
                print(f"Invalid mask shape: {binary_mask.shape}")
                continue

            try:
                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            except cv2.error as e:
                print(f"Error finding contours: {e}")
                print(f"Mask shape: {binary_mask.shape}, dtype: {binary_mask.dtype}")
                print(f"Mask min: {binary_mask.min()}, max: {binary_mask.max()}")
                continue

            segmentation = []
            for contour in contours:
                transformed_contour = []    
                for point in contour.reshape(-1, 2):    
                    x, y = point    
                    absolute_x = left + (x / width) * (right - left)    
                    absolute_y = top + (y / height) * (bottom - top)    
                    transformed_contour.extend([absolute_x, absolute_y])    
                if len(transformed_contour) > 4:
                    segmentation.append(transformed_contour)

            area  = float(mask_util.area(mask_util.encode(np.asfortranarray(binary_mask))))
            class_id = int(cls)

            annotation = {
            "id": len(coco_output["annotations"]) + 1,
            "image_id": image_id,
            "category_id": class_id,
            "bbox": absolute_box,
            "area": area,
            "segmentation": segmentation,
            "iscrowd": 0
        }
            coco_output["annotations"].append(annotation)
        
    with open(output_file, 'w') as f:
        json.dump(coco_output, f, separators=(',', ':'))


def visualize_coco_annotations(image_path, json_path):
    """
    Visualizes COCO annotations on an image.

    Args:
        image_path (str): Path to the input image file.
        json_path (str): Path to the COCO JSON file containing annotations.

    Returns:
        numpy.ndarray: The image with visualized annotations.

    This function performs the following steps:
    1. Loads the COCO annotations from the JSON file.
    2. Reads and converts the input image to RGB format.
    3. Iterates through the annotations and draws on the image:
       - Bounding boxes are drawn in red.
       - Segmentation polygons are drawn in blue.

    Note:
        - Bounding boxes are drawn with a thickness of 1 pixel.
        - Segmentation polygons are drawn with a thickness of 1 pixel.

    Warning:
        Ensure that the input image path and JSON file path are correct and accessible.
    """
    with open(json_path, 'r') as f:
        coco_data = json.load(f)
    
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Draw annotations on the image
    for ann in coco_data['annotations']:
        bbox = ann['bbox']
        x, y, w, h = [int(coord) for coord in bbox]
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)  # Red for our annotations
        
        if 'segmentation' in ann and ann['segmentation']:
            for seg in ann['segmentation']:
                pts = np.array(seg).reshape((-1, 1, 2)).astype(np.int32)
                cv2.polylines(image, [pts], True, (0, 0, 255), 1)  # Blue for segmentation
    
    return image


def main():
    CONFIDENCE_THRESHOLD = 0.60
    # This for executing in the local machine
    # IMAGE_PATH = '/workspace/COCO_DATASET/scripts/000000001000.jpg'
    # ANNOTATIONS_PATH = '/workspaces/herta/COCO_DATASET/annotations_trainval2017/annotations/instances_val2017.json'
    # MODEL_PATH = "/workspace/COCO_DATASET/scripts/segment_clothes.engine"
    # This for the container
    IMAGE_PATH = '/home/COCO/scripts/000000001000.jpg'
    ANNOTATIONS_PATH = '/home/COCO/annotations_trainval2017/annotations/instances_val2017.json'
    MODEL_PATH = "/home/COCO/scripts/segment_clothes.engine"
    OUTPUT_DIR = 'extracted_segments'
    COCO_OUTPUT_FILE = 'output_annotations.json'
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    model = YOLO(MODEL_PATH, task='segment')
    
    image, coco, img_id = load_image_and_coco_annotations(IMAGE_PATH, ANNOTATIONS_PATH)
    
    anns_person = get_annotations(coco, img_id, ['person'])
    anns_bag = get_annotations(coco, img_id, ['backpack', 'handbag', 'suitcase'])
    
    people_with_bags = associate_bags_with_people(anns_person, anns_bag)
    
    full_segmentation, all_results = extract_and_process_segments(
        image, coco, people_with_bags, model, CONFIDENCE_THRESHOLD, OUTPUT_DIR
    )
    
    full_segmentation_path = os.path.join(OUTPUT_DIR, 'full_segmentation.png')
    cv2.imwrite(full_segmentation_path, cv2.cvtColor(full_segmentation, cv2.COLOR_RGB2BGR))
    print(f"Complete segmentation saved in: {full_segmentation_path}")
    # Create and save COCO annotations
    create_coco_annotations(IMAGE_PATH, all_results, os.path.join(OUTPUT_DIR, COCO_OUTPUT_FILE))
    print(f"COCO's annotations saved in: {os.path.join(OUTPUT_DIR, COCO_OUTPUT_FILE)}")
    
    print(f"Original Image: {os.path.basename(IMAGE_PATH)}")
    print(f"People with bags detected: {len(people_with_bags)}")

    visualized_image = visualize_coco_annotations(IMAGE_PATH, os.path.join(OUTPUT_DIR, COCO_OUTPUT_FILE))
    cv2.imwrite(os.path.join(OUTPUT_DIR, 'visualized_coco_annotations2.jpg'), cv2.cvtColor(visualized_image, cv2.COLOR_RGB2BGR))

if __name__ == "__main__":
    main()
```

Este para directorios:
```Python
import cv2
import numpy as np
from pycocotools.coco import COCO
import os
from ultralytics import YOLO
from PIL import Image
import json
import datetime
import torchvision.transforms.functional as F
import torch
from pycocotools import mask as mask_util


def calculate_iou(box1, box2):
    """
    Calculate the Intersection over Union (IoU) of two bounding boxes.

    @param box1 List of 4 numbers representing the first bounding box [x, y, width, height].
    @param box2 List of 4 numbers representing the second bounding box [x, y, width, height].
    @return float The IoU value between 0 and 1.

    If there's no intersection, the function returns 0.0.
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[0] + box1[2], box2[0] + box2[2])
    y2 = min(box1[1] + box1[3], box2[1] + box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = box1[2] * box1[3]
    area2 = box2[2] * box2[3]
    
    iou = intersection / float(area1 + area2 - intersection)
    return iou


def resize_with_padding(person, target_size):
    """
    Resize an image with padding to reach a target size while maintaining aspect ratio.

    @param person PIL.Image The input image to be resized.
    @param target_size tuple A tuple of (width, height) representing the desired output size.
    @return tuple A tuple containing:
        - PIL.Image: The resized and padded image.
        - float: The ratio used for resizing.

    Note: The function assumes a target size of 512x512 for ratio calculation.
    The cv2 and numpy libraries are used for saving the image.

    @todo Consider making the target size (512x512) a parameter instead of a hard-coded value.
    """
    w, h = person.size
    ratio = min(512 / w, 512 / h)
    new_size = (int(w * ratio), int(h * ratio))
    resized_image = person.resize(new_size, Image.LANCZOS)
    new_image = Image.new('RGB', target_size, (0, 0, 0))
    new_image.paste(resized_image, (0, 0))    
    return new_image, ratio

def load_image_and_coco_annotations(image_path, annotations_path):
    """
    Load an image and its corresponding COCO annotations.

    @param image_path str The file path to the image.
    @param annotations_path str The file path to the COCO annotations JSON file.
    @return tuple A tuple containing:
        - numpy.ndarray: The loaded image in RGB format.
        - pycocotools.coco.COCO: The COCO object with loaded annotations.
        - int: The image ID in the COCO dataset.


    @throws ValueError If the image is not found in the COCO annotations.

    @note This function requires the pycocotools, cv2, and os libraries.
    @note The image is converted from BGR to RGB color space.
    """
    coco = COCO(annotations_path)
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    file_name = os.path.basename(image_path)
    img_id = next((id for id in coco.getImgIds() if coco.loadImgs(id)[0]['file_name'] == file_name), None)
    
    if img_id is None:
        raise ValueError(f"Image not fount {file_name} in COCO's annotations.")
    
    return image, coco, img_id

def get_annotations(coco, img_id, categories):
    """
    Retrieve annotations for a specific image and categories from a COCO dataset.

    @param coco pycocotools.coco.COCO The COCO object containing the dataset annotations.
    @param img_id int The ID of the image for which to retrieve annotations.
    @param categories list A list of category names to filter the annotations.
    @return list A list of annotation dictionaries matching the specified image and categories.

    @note This function assumes that the COCO object is properly initialized with the dataset.
    @note The returned annotations are in the COCO format, which typically includes fields like
          'segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', and 'id'.
    """
    cat_ids = coco.getCatIds(catNms=categories)
    ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids)
    return coco.loadAnns(ann_ids)

def associate_bags_with_people(anns_person, anns_bags, iou_threshold=0.1):
    """
    Associate bags with people based on the Intersection over Union (IoU) of their bounding boxes.

    @param anns_person list A list of person annotations, each containing a 'bbox' key.
    @param anns_bags list A list of bag annotations, each containing a 'bbox' key and an 'id' key.
    @param iou_threshold float The minimum IoU value to consider a bag associated with a person. Default is 0.1.
    @return list A list of tuples, each containing a person annotation and a list of associated bag annotations.

    @note The function assumes the existence of a 'calculate_iou' function to compute the IoU between bounding boxes.
    @note Each bag can only be associated with one person.
    @note The bounding boxes are expected to be in the format [x, y, width, height].

    @see calculate_iou() for the IoU calculation implementation.
    """
    person_with_bags = []
    associated_bags = set()
    
    for ann_person in anns_person:
        bbox_person = ann_person['bbox']
        bags_from_person = []
        
        for ann_bag in anns_bags:
            if ann_bag['id'] in associated_bags:
                continue
            
            bbox_bag = ann_bag['bbox']
            iou = calculate_iou(bbox_person, bbox_bag)
            
            if iou > iou_threshold:
                bags_from_person.append(ann_bag)
                associated_bags.add(ann_bag['id'])
        
        person_with_bags.append((ann_person, bags_from_person))
    
    return person_with_bags

def apply_yolo_with_confidence(model, image, mask, bbox, conf_threshold=0.25):
    """
    Apply YOLO object detection to a specific region of an image, considering a mask and confidence threshold.

    @param model The YOLO model to use for detection.
    @param image numpy.ndarray The full input image.
    @param mask numpy.ndarray A binary mask of the same size as the image.
    @param bbox tuple Bounding box coordinates (left, top, right, bottom) of the region of interest.
    @param conf_threshold float Confidence threshold for YOLO detections. Default is 0.25.

    @return tuple A tuple containing:
        - numpy.ndarray: Overlay image with detection visualizations.
        - numpy.ndarray: Segmentation mask with class IDs.
        - numpy.ndarray: Colored mask representing different detected classes.
        - dict: Detailed results including masks, boxes, classes, confidences, and metadata.

    @raises ValueError If the mask is not a numpy array.

    This function performs the following steps:
    1. Extracts the region of interest from the image and mask.
    2. Applies the mask to the region of interest.
    3. Resizes the masked region for YOLO input.
    4. Runs YOLO detection on the resized image.
    5. Processes YOLO results, including resizing masks and boxes back to original size.
    6. Creates visualizations including segmentation masks and bounding boxes.
    7. Compiles detailed results into a dictionary.

    @note This function assumes the existence of a 'resize_with_padding' function.
    @note The function uses OpenCV (cv2) for drawing and image processing operations.
    @note YOLO results are expected to include masks for segmentation.

    @todo Consider adding error handling for potential issues in YOLO detection or result processing.
    @todo Optimize the visualization process for better performance with large images or numerous detections.
    """
    if not isinstance(mask, np.ndarray):
        raise ValueError("La máscara debe ser un array numpy, no un float.")

    left, top, right, bottom = bbox
    person = image[top:bottom, left:right].copy()
    person_mask = mask[top:bottom, left:right]
    
    person[person_mask == 0] = [0, 0, 0]

    person_image = Image.fromarray(person)
    resized_person, ratio = resize_with_padding(person_image, (640, 640))
    
    results = model(resized_person, conf=conf_threshold)
    
    assert len(results) == 1, "Only one output should be returned."
    result = results[0]

    height, width = person.shape[:2]
    segmentation_mask = np.zeros((height, width), dtype=np.uint8)
    colored_mask = np.zeros((height, width, 3), dtype=np.uint8)
    overlay_segment = image[top:bottom, left:right].copy()
    
    class_colors = {}
    
    result_dict = {}

    if result.masks is not None:
        new_h, new_w = int(640/ratio), int(640/ratio)
        result_dict['masks'] = F.resize(result.masks.data, (new_h, new_w), interpolation=F.InterpolationMode.NEAREST)
        result_dict['boxes'] = result.boxes.xyxy / ratio
        result_dict['classes'] = result.boxes.cls
        result_dict['confs'] = result.boxes.conf
        result_dict['names'] = result.names
        result_dict['size'] = {'height': height, 'width': width}
        result_dict['relative_position'] = { 'left': left, 'top': top, 'right': right, 'bottom': bottom }
        result_dict['categories'] = [category for _, category in results[0].names.items()]
        for mask_yolo, box, cls, conf in zip(result_dict['masks'], result_dict['boxes'], result_dict['classes'], result_dict['confs'] ):
            class_name = result_dict['names'][int(cls)]
            
            if class_name not in class_colors:
                class_colors[class_name] = tuple(np.random.randint(0, 255, 3).tolist())
            color = class_colors[class_name]
            
            # Convert mask to original size of the person
            mask_np = mask_yolo.cpu().numpy()
            mask_np = mask_np[:height, :width]
            
            mask_np = np.logical_and(mask_np > 0.5, person_mask > 0).astype(np.uint8)
            # Apply mask to segmentation mask and colored mask
            segmentation_mask[mask_np > 0] = int(cls) + 1
            colored_mask[mask_np > 0] = color
            
            overlay_segment[mask_np > 0] = overlay_segment[mask_np > 0] * 0.5 + np.array(color) * 0.5
            
            box = box.tolist()
            
            cv2.rectangle(overlay_segment, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)
            cv2.putText(overlay_segment, f"{class_name} {conf:.2f}", (int(box[0]), int(box[1] - 10)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    return overlay_segment, segmentation_mask, colored_mask, result_dict



def extract_and_process_segments(image, coco, people_with_bags, model, conf_threshold, output_dir):
    """
    @brief Extracts and processes segments from an image based on person and bag annotations.
    
    This function takes an image and annotations for people and bags, processes each segment
    using a YOLO model, and saves the results as images and detection information.
    
    @param image The input image to process.
    @param coco A COCO annotation object for creating masks.
    @param people_with_bags A list of tuples containing person annotations and associated bag annotations.
    @param model The YOLO model used for object detection.
    @param conf_threshold Confidence threshold for YOLO detections.
    @param output_dir Directory to save output images.
    
    @return A tuple containing:
            - full_segmentation: The input image with all processed segments overlaid.
            - all_results: A list of dictionaries containing detection results for each segment.
    
    @details The function performs the following steps for each person-bag pair:
             1. Creates a mask combining the person and their associated bags.
             2. Extracts the bounding box for the combined mask.
             3. Applies YOLO detection on the masked region.
             4. Saves overlay, segmentation mask, and colored mask images.
             5. Updates the full segmentation image with the processed segment.
             6. Prints detected classes for each segment.
    
    @note Empty segments are skipped with a warning message.
    """
    full_segmentation = image.copy()
    all_results = []
    
    for i, (ann_person, bags) in enumerate(people_with_bags):
        person_mask = coco.annToMask(ann_person)
        
        for bag in bags:
            bag_mask = coco.annToMask(bag)
            person_mask = np.logical_or(person_mask, bag_mask)
        
        y, x = np.where(person_mask)
        top, bottom, left, right = y.min(), y.max(), x.min(), x.max()
        
        if top == bottom or left == right:
            print(f"Warning: Empty segment for person {i}. Skipping...")
            continue
        
        overlay_image, segmentation_mask, colored_mask, result_dict = apply_yolo_with_confidence(model, image, person_mask, (left, top, right, bottom), conf_threshold)
        
        all_results.append(result_dict)
        
        cv2.imwrite(os.path.join(output_dir, f'overlay_person_{"with" if bags else "without"}_bag_{i}.png'), cv2.cvtColor(overlay_image, cv2.COLOR_RGB2BGR))
        cv2.imwrite(os.path.join(output_dir, f'person_mask_{"with" if bags else "without"}_bag_{i}.png'), segmentation_mask * 255)
        cv2.imwrite(os.path.join(output_dir, f'colored_person_mask_{"with" if bags else "without"}_bag_{i}.png'), cv2.cvtColor(colored_mask, cv2.COLOR_RGB2BGR))
        
        full_segmentation[top:bottom, left:right] = overlay_image
        cv2.rectangle(full_segmentation, (left, top), (right, bottom), (255, 0, 0), 2)
        
        unique_classes = np.unique(segmentation_mask)
        print(f"Classes detected in segment {i}: {[model.names[cls-1] for cls in unique_classes if cls > 0]}")
    
    return full_segmentation, all_results

def create_coco_annotations(image_path, results, output_file):
    """
    Creates COCO-format annotations from YOLO detection results and existing annotations.

    Args:
        image_path (str): Path to the input image file.
        results (list): List of dictionaries containing YOLO detection results for each segment.
        output_file (str): Path to save the output COCO JSON file.

    This function performs the following steps:
    1. Initializes a COCO-format dictionary with metadata.
    2. Adds categories based on the detection results.
    3. Adds image information to the COCO structure.
    4. Processes each set of detection results:
       - Converts YOLO masks to binary masks.
       - Finds contours in the binary masks.
       - Transforms contours to absolute image coordinates.
       - Calculates areas and bounding boxes.
       - Creates COCO-format annotations for each detected object.
    5. Saves the COCO-format annotations to a JSON file.

    Note:
        - The function handles potential errors in mask processing and contour finding.
    Warning:
        Ensure that the input image and detection results correspond to each other.
    """
    coco_output = {
        "info": {
            "description": "Dataset generated with YOLOv8",
            "url": "",
            "version": "1.0",
            "year": datetime.datetime.now().year,
            "contributor": "",
            "date_created": datetime.datetime.now().isoformat()
        },
        "licenses": [{"url": "", "id": 1, "name": "Unknown"}],
        "images": [],
        "annotations": [],
        "categories": []
    }

    # Check if there are any results
    if results and len(results) > 0 and 'categories' in results[0]:
        for i, category in enumerate(results[0]['categories'], 1):
            coco_output["categories"].append({
                "id": i,
                "name": category,
                "supercategory": ""
            })
    else:
        print(f"No categories found for image: {image_path}")
        # Add a default category if no categories are found
        coco_output["categories"].append({
            "id": 1,
            "name": "unknown",
            "supercategory": ""
        })
    
    image = Image.open(image_path)
    image_id = 1
    orig_width, orig_height = image.size
    coco_output["images"].append({
        "id": image_id,
        "width": orig_width,
        "height": orig_height,
        "file_name": os.path.basename(image_path),
        "license": 1,
        "flickr_url": "",
        "coco_url": "",
        "date_captured": datetime.datetime.now().isoformat()
    })
    
    for i in range(len(results)):
        if not results[i]:
            continue
        masks_yolo = results[i].get('masks', [])
        boxes = results[i].get('boxes', []).tolist()
        cls = results[i].get('classes', []).tolist()
        conf = results[i].get('confs', []).tolist()
        height, width = results[i]['size']['height'], results[i]['size']['width']
        left, top, right, bottom = results[i]['relative_position']['left'], results[i]['relative_position']['top'], results[i]['relative_position']['right'], results[i]['relative_position']['bottom']

        for mask, box, cls, conf in zip(masks_yolo, boxes, cls, conf):
            width_box, height_box = box[2] - box[0], box[3] - box[1]
            absolute_box = [left + box[0], top + box[1], width_box, height_box]

            if isinstance(mask, torch.Tensor):
                mask_np = mask.cpu().numpy()
                mask_np = mask_np[:height, :width]
            elif isinstance(mask, np.ndarray):
                mask_np = mask

            if mask_np.ndim > 2:
                mask_np = mask_np.squeeze()
        
            binary_mask = (mask_np > 0.5).astype(np.uint8) * 255

            if binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0:
                print(f"Invalid mask shape: {binary_mask.shape}")
                continue

            try:
                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            except cv2.error as e:
                print(f"Error finding contours: {e}")
                print(f"Mask shape: {binary_mask.shape}, dtype: {binary_mask.dtype}")
                print(f"Mask min: {binary_mask.min()}, max: {binary_mask.max()}")
                continue

            segmentation = []
            for contour in contours:
                transformed_contour = []    
                for point in contour.reshape(-1, 2):    
                    x, y = point    
                    absolute_x = left + (x / width) * (right - left)    
                    absolute_y = top + (y / height) * (bottom - top)    
                    transformed_contour.extend([absolute_x, absolute_y])    
                if len(transformed_contour) > 4:
                    segmentation.append(transformed_contour)

            area  = float(mask_util.area(mask_util.encode(np.asfortranarray(binary_mask))))
            class_id = int(cls)

            annotation = {
            "id": len(coco_output["annotations"]) + 1,
            "image_id": image_id,
            "category_id": class_id,
            "bbox": absolute_box,
            "area": area,
            "segmentation": segmentation,
            "iscrowd": 0
        }
            coco_output["annotations"].append(annotation)
        
    with open(output_file, 'w') as f:
        json.dump(coco_output, f, separators=(',', ':'))


def visualize_coco_annotations(image_path, json_path):
    """
    Visualizes COCO annotations on an image.

    Args:
        image_path (str): Path to the input image file.
        json_path (str): Path to the COCO JSON file containing annotations.

    Returns:
        numpy.ndarray: The image with visualized annotations.

    This function performs the following steps:
    1. Loads the COCO annotations from the JSON file.
    2. Reads and converts the input image to RGB format.
    3. Iterates through the annotations and draws on the image:
       - Bounding boxes are drawn in red.
       - Segmentation polygons are drawn in blue.

    Note:
        - Bounding boxes are drawn with a thickness of 1 pixel.
        - Segmentation polygons are drawn with a thickness of 1 pixel.

    Warning:
        Ensure that the input image path and JSON file path are correct and accessible.
    """
    with open(json_path, 'r') as f:
        coco_data = json.load(f)
    
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Draw annotations on the image
    for ann in coco_data['annotations']:
        bbox = ann['bbox']
        x, y, w, h = [int(coord) for coord in bbox]
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 1)  # Red for our annotations
        
        if 'segmentation' in ann and ann['segmentation']:
            for seg in ann['segmentation']:
                pts = np.array(seg).reshape((-1, 1, 2)).astype(np.int32)
                cv2.polylines(image, [pts], True, (0, 0, 255), 1)  # Blue for segmentation
    
    return image


def process_single_image(image_path, annotation_path, model, conf_threshold, output_dir):
    """
    Process a single image with the given COCO annotations and YOLO model.
    
    Args:
        image_path (str): Path to the image file.
        annotation_path (str): Path to the COCO annotation file.
        model (YOLO): YOLO model for object detection.
        conf_threshold (float): Confidence threshold for YOLO detections.
        output_dir (str): Directory to save output files.
    
    Returns:
        dict: Results of processing the image.
    """
    try:
        image, coco, img_id = load_image_and_coco_annotations(image_path, annotation_path)
        
        anns_person = get_annotations(coco, img_id, ['person'])
        anns_bag = get_annotations(coco, img_id, ['backpack', 'handbag', 'suitcase'])
        
        people_with_bags = associate_bags_with_people(anns_person, anns_bag)
        
        full_segmentation, all_results = extract_and_process_segments(
            image, coco, people_with_bags, model, conf_threshold, output_dir
        )
        
        image_name = os.path.basename(image_path)
        full_segmentation_path = os.path.join(output_dir, f'full_segmentation_{image_name}')
        cv2.imwrite(full_segmentation_path, cv2.cvtColor(full_segmentation, cv2.COLOR_RGB2BGR))
        
        coco_output_file = f'output_annotations_{image_name.split(".")[0]}.json'
        create_coco_annotations(image_path, all_results, os.path.join(output_dir, coco_output_file))
        
        visualized_image = visualize_coco_annotations(image_path, os.path.join(output_dir, coco_output_file))
        cv2.imwrite(os.path.join(output_dir, f'visualized_coco_annotations_{image_name}'), cv2.cvtColor(visualized_image, cv2.COLOR_RGB2BGR))
        
        return {
            'image_name': image_name,
            'people_with_bags': len(people_with_bags),
            'full_segmentation_path': full_segmentation_path,
            'coco_output_file': coco_output_file
        }
    except Exception as e:
        print(f"Error processing {os.path.basename(image_path)}: {str(e)}")
        return None

def main():
    CONFIDENCE_THRESHOLD = 0.10
    # Update these paths for your specific setup
    IMAGE_DIR = '/home/COCO/scripts'  # Directory containing all images to process
    ANNOTATIONS_PATH = '/home/COCO/annotations_trainval2017/annotations/instances_val2017.json'
    MODEL_PATH = "/home/COCO/scripts/segment_clothes.engine"
    OUTPUT_DIR = 'extracted_segments'
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    model = YOLO(MODEL_PATH, task='segment')
    
    # Process all images in the directory
    for image_file in os.listdir(IMAGE_DIR):
        if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
            image_path = os.path.join(IMAGE_DIR, image_file)
            print(f"Processing image: {image_file}")
            
            result = process_single_image(image_path, ANNOTATIONS_PATH, model, CONFIDENCE_THRESHOLD, OUTPUT_DIR)
            if result:
                print(f"Processed {result['image_name']}:")
                print(f"  People with bags detected: {result['people_with_bags']}")
                print(f"  Full segmentation saved: {result['full_segmentation_path']}")
                print(f"  COCO annotations saved: {result['coco_output_file']}")
            print("---------------------------")
    
    print("All images processed.")

if __name__ == "__main__":
    main()
```

Queda por hacer una prueba con el dataset de validación, prueba hecha, cosas que añadir al script para que vaya correctamente:
- [ ] Hacer que solo haga un fichero de json
- [ ] Hacer que las categorias empiecen a partir de 80
- [ ] Añadir campos que actualmente se encuentran vacíos, recuperarlos del dataset de coco (coco_url, date_captured, flickr_url, licencias, contributor, etc...)
- [ ] Ejecutar sobre el dataset de validación
- [ ] Ejecutar sobre el dataset de entrenamiento 
- [ ] Preparar un entrenamiento con estos datasets
- [ ] Probar a lanzar entrenamiento y ver tensorboard a la vez para poder monitorizar
- [ ] Documentar todo
- [ ] Monitorizar entrenamiento
- [ ] Hacer pruebas y guardar pesos cada ciertos epochs en el servidor
- [ ] Montar contenedor de docker con volumen para que guarde los entrenamientos en local, no en el servidor

Hacerlo con el dataset entero de coco
Una vez tengamos el dataset de coco retaggeado volver a etiquetar
