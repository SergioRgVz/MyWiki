# Herta
- [ ] Mirar scripts de entrenamiento run_training.py en servidor ubunturesearch
- [ ] Entender como lanzar un entrenamiento
- [ ] Intentar hacer la prueba en un contenedor docker

# Estructura Backbone-Head en Proyecto de Reconocimiento Facial/Demográfico

## 1. Backbone: "demog"

- **Función**: Extracción de características especializadas para análisis demográfico.
- **Características**:
  - Capas convolucionales diseñadas para capturar rasgos faciales relevantes para demografía (edad, género, etnia).
  - Posiblemente preentrenado en grandes datasets de rostros diversos.
  - Optimizado para mantener información demográfica en las características extraídas.

## 2. Head: "L2_Softmax"

- **Función**: Clasificación final basada en las características extraídas por el backbone.
- **Características**:
  - Utiliza normalización L2 en las características antes de aplicar softmax.
  - Ayuda a mejorar la estabilidad del entrenamiento y la generalización del modelo.
  - Particularmente útil en tareas de reconocimiento facial para manejar variaciones en la iluminación y pose.

## 3. Head Logits: "ArcFace_logits"

- **Función**: Mejora la discriminación entre clases en el espacio de características.
- **Características**:
  - Implementa la función de pérdida ArcFace, que introduce un margen angular entre clases.
  - Aumenta la separabilidad de las características, crucial en reconocimiento facial.
  - Ayuda a manejar casos difíciles donde las diferencias entre identidades son sutiles.

## Ventajas de esta Estructura en tu Proyecto

1. **Especialización**: El backbone "demog" está probablemente optimizado para extraer características relevantes para análisis demográfico, mejorando la precisión en estas tareas.

2. **Flexibilidad**: La separación entre backbone y head permite ajustar fácilmente el modelo para diferentes tareas (por ejemplo, cambiar entre clasificación de edad y reconocimiento de identidad).

3. **Rendimiento Mejorado**: La combinación de L2_Softmax y ArcFace_logits sugiere un enfoque en alta precisión y robustez, especialmente importante en aplicaciones de reconocimiento facial.

4. **Transferencia de Aprendizaje**: Facilita el uso de backbones preentrenados, útil si tienes limitaciones de datos para ciertas tareas demográficas.

5. **Experimentación**: Permite probar fácilmente diferentes configuraciones de head manteniendo el mismo backbone, o viceversa.

## Consideraciones Adicionales

- **Balanceo de Datos**: La configuración `balancer_type: "id"` sugiere que estás balanceando los datos por identidad, lo cual es crucial en reconocimiento facial para evitar sesgos.

- **Aumento de Datos**: El uso de transformaciones como `RandomCrop`, `CenterCrop`, y `RandomHorizontalFlip` indica un enfoque en la robustez del modelo frente a variaciones en la posición y orientación facial.

- **GAN DA**: La configuración de GANs sugiere el uso de técnicas avanzadas de aumento de datos, posiblemente para generar más variedad en las características demográficas de las imágenes de entrenamiento.

Esta estructura backbone-head, junto con las técnicas adicionales implementadas, indica un enfoque sofisticado y bien pensado para abordar desafíos en reconocimiento facial y análisis demográfico.

# Datasets
Tienen 4 principales:
- LFW: useful to see that the model is converging correctly. Based on experience, in 3/4 epochs at maximum, the model should reach around 0.9 of AUC43.
- DemogPairs: useful to see how the model starts to overfit. DemogPairs has a very similar distribution to the training distribution, as both datasets are made of celebrities' images.
- MEDS: gives a first idea of how well a model will perform in real cases. Therefore, the checkpoint with the best AUC43 in MEDS should be kept. If the performance is very poor in MEDS, it is another indicator that there might be something wrong with the hyperparameter choosing or the training itself.
- IJB-C: used as a final test, it is a very complete evaluation.

# Preguntas
- ¿Qué son los archivos .fids dentro de los datasets?
- ¿Wiki de los datasets?
- ¿Wiki de cómo es el backbone-head de los modelos?
- ¿Cómo me exporto el entorno de train para probarlo?


# Done

- Descargado parte del dataset de paperdoll
- Montado docker en mi máquina y familiarizado con ello
- Montado dockerfile en ruta /home/herty/turing/home/herty/entrenador de la máquina de ubunturesearch para poder levantar un entorno conda e instalar los requirements.txt del entrenamiento sudo docker build -t train_docker .

# Para mañana
- Intentar lanzar ese entorno docker con el entorno de conda en el servidor
- Copiar carpeta de entrenador a mi maquina
- Montar contenedor docker en mi máquina para lanzar entrenamiento